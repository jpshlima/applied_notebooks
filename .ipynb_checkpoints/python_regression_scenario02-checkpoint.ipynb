{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us make the imports for the entire code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "\n",
    "# Enable to start counting processing time\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_output(row):\n",
    "    # if downloadTime is different than 100 for T2 and T3, both have completed the download\n",
    "    if ((row.downloadTimeT2!=100)&(row.downloadTimeT3!=100)):\n",
    "        # the best output has smaller downloadTime\n",
    "        if (row.downloadTimeT2<=row.downloadTimeT3):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # if downloadTime is different than 100 for only one target, only one completes the download\n",
    "    elif ((row.downloadTimeT2!=100)|(row.downloadTimeT3!=100)):\n",
    "        # the best output has downloadTime other than 100 (completed download before simulation time ends)\n",
    "        if (row.downloadTimeT2!=100):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # if downloadTime = 100 for both T2 and T3, both targets does not complete download\n",
    "    elif ((row.downloadTimeT2==100)&(row.downloadTimeT3==100)):\n",
    "        # the best output has greater rxBytes\n",
    "        if (row.rxBytesT2>=row.rxBytesT3):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we want to select rxBytes data from the eNB indicated by best_output function\n",
    "def rxbytes(row):\n",
    "    if (row.best_output == 1):\n",
    "        return int(row.rxBytesT3)\n",
    "    else:\n",
    "        return int(row.rxBytesT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read and organize CSV data\n",
    "\n",
    "t2 = pd.read_csv('t2_OhBuildings_ComShadowing_Modificado', delimiter='\\t')\n",
    "t3 = pd.read_csv('t3_OhBuildings_ComShadowing_Modificado', delimiter='\\t')\n",
    "\n",
    "t2 = t2[t2.nRun.isin(t3.nRun)]\n",
    "t2.reset_index(drop=True, inplace=True)\n",
    "t3 = t3[t3.nRun.isin(t2.nRun)]\n",
    "t3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "a3rsrp = pd.read_csv('A3RSRP_OhBuildings_ComShadowing_Modificado', delimiter='\\t')\n",
    "a2a4rsrq = pd.read_csv('A2A4RSRQ_OhBuildings_ComShadowing_Modificado', delimiter='\\t')\n",
    "\n",
    "\n",
    "# Guarantee that we utilize only seeds present in both datasets\n",
    "t2_runs = t2.nRun\n",
    "t3_runs = t3.nRun\n",
    "a3rsrp_runs = a3rsrp.nRun\n",
    "a2a4rsrq_runs = a2a4rsrq.nRun\n",
    "valid_results = t2[t2.rsrp1==t3.rsrp1].nRun\n",
    "valid_runs = set(t2_runs).intersection(t3_runs).intersection(a3rsrp_runs).intersection(a2a4rsrq_runs).intersection(valid_results)\n",
    "\n",
    "t2 = t2[t2.nRun.isin(valid_runs)]\n",
    "t3 = t3[t3.nRun.isin(valid_runs)]\n",
    "a3rsrp = a3rsrp[a3rsrp.nRun.isin(valid_runs)]\n",
    "a2a4rsrq = a2a4rsrq[a2a4rsrq.nRun.isin(valid_runs)]\n",
    "\n",
    "t2.reset_index(drop=True, inplace=True)\n",
    "t3.reset_index(drop=True, inplace=True)\n",
    "a3rsrp.reset_index(drop=True, inplace=True)\n",
    "a2a4rsrq.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combining datasets\n",
    "data = t2\n",
    "data = data.drop(['targetCellId', 'downloadTime', 'rxBytes'], axis=1)\n",
    "data['downloadTimeT2'] = t2.downloadTime\n",
    "data['downloadTimeT3'] = t3.downloadTime\n",
    "data['rxBytesT2'] = t2.rxBytes\n",
    "data['rxBytesT3'] = t3.rxBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'downloadTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-811736fa2c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'best_output'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Applies our function to select which downloadTime will be used for regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'downloadTime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownloadTime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'downloadTime' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "# Applies our function defined above to obtain the best output\n",
    "data['best_output'] = data.apply(best_output, axis=1)\n",
    "# Applies our function to select which downloadTime will be used for regression\n",
    "data['downloadTime'] = data.apply(downloadTime, axis=1)\n",
    "\n",
    "\n",
    "# Sets data as inputs and labels\n",
    "previsores = data[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1','previousrsrq1','previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "previsores = previsores.values\n",
    "label = (data[['bytes']]/15728640) * 100\n",
    "label = label.values\n",
    "\n",
    "\n",
    "# Scaling data\n",
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "previsores = scaler_x.fit_transform(previsores)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "label = scaler_y.fit_transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# KNN\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    matriz1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the KNN regressor \n",
    "        regressor = neighbors.KNeighborsRegressor(n_neighbors = 12)\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    matriz1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the XGBoost regressor \n",
    "        regressor = MLPRegressor(\n",
    "                                    activation = 'tanh',\n",
    "                                    alpha = 0.001,\n",
    "                                    hidden_layer_sizes = 7,\n",
    "                                    learning_rate = 'invscaling',\n",
    "                                    learning_rate_init = 0.0199843120298352,\n",
    "                                    max_iter = 2800,\n",
    "                                )\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# Random Forest \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    matriz1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the Random Forest regressor \n",
    "        regressor = RandomForestRegressor(\n",
    "                                            criterion = 'mse',\n",
    "                                            max_depth = 6,\n",
    "                                            max_features = 0.7,\n",
    "                                            max_samples = 0.8,\n",
    "                                            n_estimators = 94,\n",
    "                                         )\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "#Gradient Boosting Machine (GBM)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    matriz1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the GBM regressor \n",
    "        regressor = GradientBoostingRegressor(\n",
    "                                                criterion = 'mse',\n",
    "                                                learning_rate = 0.047744243757850054,\n",
    "                                                max_depth = 4,\n",
    "                                                max_features = 0.4,\n",
    "                                                n_estimators = 120,\n",
    "                                                subsample = 0.7,\n",
    "                                                )\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "#LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the LightGBM regressor \n",
    "        regressor = lightgbm.LGBMRegressor(\n",
    "                                            bagging_fraction = 0.4,\n",
    "                                            eval_metric = 'mae',\n",
    "                                            feature_fraction = 0.7,\n",
    "                                            learning_rate = 0.033515974245406554,\n",
    "                                            max_depth = 12,\n",
    "                                            min_data_in_leaf = 36,\n",
    "                                            n_estimators = 139,\n",
    "                                            objective = 'regression_l1',\n",
    "                                            )\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the XGBoost regressor \n",
    "        regressor = xgb.XGBRegressor (\n",
    "                                        booster = 'gbtree',\n",
    "                                        colsample_bylevel = 0.7,\n",
    "                                        colsample_bynode = 0.8,\n",
    "                                        colsample_bytree = 0.8,\n",
    "                                        eval_metric = 'mae',\n",
    "                                        learning_rate = 0.04492984940391834,\n",
    "                                        max_depth = 7,\n",
    "                                        n_estimators = 165,\n",
    "                                        objective = 'reg:logistic'\n",
    "                                    )\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
