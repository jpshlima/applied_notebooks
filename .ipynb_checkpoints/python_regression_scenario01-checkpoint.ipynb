{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us make the imports for the entire code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "# Enable to start counting processing time\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_output(row):\n",
    "    # if downloadTime is different than 100 for T2 and T3, both have completed the download\n",
    "    if ((row.downloadTimeT2!=100)&(row.downloadTimeT3!=100)):\n",
    "        # the best output has smaller downloadTime\n",
    "        if (row.downloadTimeT2<=row.downloadTimeT3):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # if downloadTime is different than 100 for only one target, only one completes the download\n",
    "    elif ((row.downloadTimeT2!=100)|(row.downloadTimeT3!=100)):\n",
    "        # the best output has downloadTime other than 100 (completed download before simulation time ends)\n",
    "        if (row.downloadTimeT2!=100):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # if downloadTime = 100 for both T2 and T3, both targets does not complete download\n",
    "    elif ((row.downloadTimeT2==100)&(row.downloadTimeT3==100)):\n",
    "        # the best output has greater rxBytes\n",
    "        if (row.rxBytesT2>=row.rxBytesT3):\n",
    "            return 2\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to select the lowest download time to estimate, since this will be the chosen one\n",
    "def downloadTime(row):\n",
    "    # if the best_output function tells us the best choice is eNB 3, we select its downloadTime data\n",
    "    if (row.best_output == 1):\n",
    "        return float(row.downloadTimeT3)\n",
    "    else:\n",
    "        return float(row.downloadTimeT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read and organize CSV data\n",
    "\n",
    "t2 = pd.read_csv('t2_OkumuraHata_Modificado', delimiter='\\t')\n",
    "t3 = pd.read_csv('t3_OkumuraHata_Modificado', delimiter='\\t')\n",
    "\n",
    "# Guarantee that we utilize only seeds present in both datasets\n",
    "t2 = t2[t2.nRun.isin(t3.nRun)]\n",
    "t3 = t3[t3.nRun.isin(t2.nRun)]\n",
    "t2 = t2.reset_index(drop=True)\n",
    "t3 = t3.reset_index(drop=True)\n",
    "\n",
    "# Combining datasets\n",
    "data = t2\n",
    "data = data.drop(['targetCellId', 'downloadTime', 'rxBytes'], axis=1)\n",
    "data['downloadTimeT2'] = t2.downloadTime\n",
    "data['downloadTimeT3'] = t3.downloadTime\n",
    "data['rxBytesT2'] = t2.rxBytes\n",
    "data['rxBytesT3'] = t3.rxBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "# Applies our function defined above to obtain the best output\n",
    "data['best_output'] = data.apply(best_output, axis=1)\n",
    "# Applies our function to select which downloadTime will be used for regression\n",
    "data['downloadTime'] = data.apply(downloadTime, axis=1)\n",
    "\n",
    "# Sets data as inputs and labels\n",
    "previsores = data[['rsrp1','rsrq1','rsrp2','rsrq2','rsrp3','rsrq3','previousrsrp1','previousrsrq1','previousrsrp2','previousrsrq2','previousrsrp3','previousrsrq3']]\n",
    "previsores = previsores.values\n",
    "label = data[['downloadTime']] \n",
    "label = label.values\n",
    "\n",
    "# Scaling data\n",
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "previsores = scaler_x.fit_transform(previsores)\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "label = scaler_y.fit_transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12900977651515164"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# KNN\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the KNN regressor \n",
    "        regressor = neighbors.KNeighborsRegressor(n_neighbors = 4)\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27060398832365123"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the MLP regressor \n",
    "        regressor = MLPRegressor(activation = 'tanh',\n",
    "                                 hidden_layer_sizes = 22,\n",
    "                                 learning_rate = 'invscaling',\n",
    "                                 learning_rate_init = 0.03026389988096674,\n",
    "                                 max_iter = 5700,\n",
    "                                 solver = 'lbfgs')\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12272766963918438"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the Random Forest regressor \n",
    "        regressor = RandomForestRegressor(criterion = 'mse',\n",
    "                                          max_depth = 9,\n",
    "                                          max_features = 0.4,\n",
    "                                          n_estimators = 106) \n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12809262766581764"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# Gradient Boosting Machine (GBM)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the GBM regressor \n",
    "        regressor = GradientBoostingRegressor(criterion = 'mse',\n",
    "                                              learning_rate = 0.06856362171992,\n",
    "                                              max_depth = 6,\n",
    "                                              max_features = 0.5,\n",
    "                                              n_estimators = 84,\n",
    "                                              subsample = 0.8)\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7e4c54ce9492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# LightGBM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the LightGBM regressor \n",
    "        regressor = lightgbm.LGBMRegressor(objective = 'regression_l1',\n",
    "                                           bagging_fraction = 0.8,\n",
    "                                           eval_metric = 'mae',\n",
    "                                           feature_fraction = 0.4,\n",
    "                                           learning_rate = 0.08165872333050837,\n",
    "                                           max_depth = 9,\n",
    "                                           n_estimators = 148)\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1b053e194226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# XGBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Now we load our regressor, execute k-Fold, train and test our algorithm\n",
    "\n",
    "# XGBoost \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Applies StratifiedKFold with k = 5 and repeats process 33 times for statistical robustness\n",
    "resultados33 = []\n",
    "\n",
    "for i in range(33):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = i)\n",
    "    resultados1 = []\n",
    "    for n_train, n_test in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "        # Train the XGBoost regressor \n",
    "        regressor = xgb.XGBRegressor(colsample_bylevel = 0.7,\n",
    "                                     colsample_bynode = 0.8,\n",
    "                                     colsample_bytree = 0.5,\n",
    "                                     eval_metric = 'mae',\n",
    "                                     learning_rate = 0.04638617378157029,\n",
    "                                     max_depth = 6,\n",
    "                                     n_estimators = 174,\n",
    "                                     objective = 'reg:squarederror')\n",
    "        # Fitting and prediction \n",
    "        regressor.fit(previsores[n_train], label[n_train].ravel())\n",
    "        previsoes = regressor.predict(previsores[n_test])\n",
    "        # Applying the inverse scale\n",
    "        valores_previsao = np.asarray(previsoes).reshape(-1,1)\n",
    "        valores_previsao = scaler_y.inverse_transform(valores_previsao) \n",
    "        y_teste = label[n_test].tolist()\n",
    "        y_teste = scaler_y.inverse_transform(label[n_test]) \n",
    "        # Calculating the mean absolute error (MAE)\n",
    "        mae = mean_absolute_error(y_teste, valores_previsao)   \n",
    "        resultados1.append(mae)\n",
    "    # Appending all the steps\n",
    "    resultados1 = np.asarray(resultados1)\n",
    "    media = resultados1.mean()\n",
    "    resultados33 = np.append(resultados33, media)\n",
    "# Final results\n",
    "resultados33 = np.asarray(resultados33)\n",
    "\n",
    "# Enable to obtain processing time\n",
    "# end = time.time()\n",
    "# tempo = end - start\n",
    "\n",
    "# Enable to display classification mean and standard deviation\n",
    "resultados33.mean()\n",
    "# resultados33.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
